#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šæµ‹è¯•ä¸€é£å ‚åº—é“ºä¿¡æ¯çˆ¬å–åŠŸèƒ½
Test script for Ippudo store scraping functionality
"""

import sys
import os
import json
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scraper import RamenScraper


def test_ippudo_stores(quick_mode=False):
    """æµ‹è¯•ä¸€é£å ‚åº—é“ºçˆ¬å–åŠŸèƒ½"""
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•ä¸€é£å ‚åº—é“ºä¿¡æ¯çˆ¬å–åŠŸèƒ½")
    print("=" * 80)
    print()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = RamenScraper()
    
    # æµ‹è¯•å•ä¸ªéƒ½é“åºœçœŒï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    test_prefecture_url = "https://stores.ippudo.com/en/japan/æ±äº¬éƒ½"
    
    if quick_mode:
        print(f"ğŸ“Œ æµ‹è¯•æ¨¡å¼ï¼šä»…çˆ¬å–å•ä¸ªéƒ½é“åºœçœŒ")
        print(f"   æµ‹è¯•URL: {test_prefecture_url}")
        print()
        
        # æµ‹è¯•é€’å½’çˆ¬å–å•ä¸ªéƒ½é“åºœçœŒ
        visited_urls = set()
        stores = scraper.scrape_ippudo_stores_recursive(
            test_prefecture_url,
            "æ±äº¬éƒ½",
            visited_urls,
            max_depth=5
        )
        
        print(f"\n{'='*80}")
        print(f"âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"   æ‰¾åˆ° {len(stores)} å®¶åº—é“º")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªåº—é“º
        if stores:
            print("\nå‰5å®¶åº—é“ºä¿¡æ¯ï¼š")
            for i, store in enumerate(stores[:5], 1):
                print(f"\n{i}. {store['store_name']}")
                print(f"   URL: {store['url']}")
                print(f"   å†…å®¹é¢„è§ˆ: {store['content'][:100]}...")
    else:
        print("ğŸ“Œ å®Œæ•´æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰éƒ½é“åºœçœŒçš„åº—é“º")
        print()
        
        # æµ‹è¯•å®Œæ•´çˆ¬å–
        scraper.scrape_ippudo_stores()
        
        # ç»Ÿè®¡ç»“æœ
        ippudo_stores = [article for article in scraper.articles 
                        if 'ippudo' in article.get('tags', []) 
                        and article.get('section') == 'Store Information']
        
        print(f"\n{'='*80}")
        print(f"âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"   æ€»å…±æ‰¾åˆ° {len(ippudo_stores)} å®¶ä¸€é£å ‚åº—é“º")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if ippudo_stores:
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
            
            # æŒ‰éƒ½é“åºœçœŒç»Ÿè®¡ï¼ˆä»URLä¸­æå–ï¼‰
            prefecture_count = {}
            for store in ippudo_stores:
                url = store.get('url', '')
                if '/japan/' in url:
                    # æå–éƒ½é“åºœçœŒåç§°
                    parts = url.split('/japan/')
                    if len(parts) > 1:
                        prefecture = parts[1].split('/')[0]
                        prefecture_count[prefecture] = prefecture_count.get(prefecture, 0) + 1
                else:
                    prefecture_count['å…¶ä»–'] = prefecture_count.get('å…¶ä»–', 0) + 1
            
            print(f"\næŒ‰éƒ½é“åºœçœŒåˆ†å¸ƒï¼š")
            for prefecture, count in sorted(prefecture_count.items(), key=lambda x: x[1], reverse=True):
                print(f"  {prefecture}: {count} å®¶")
            
            # æ˜¾ç¤ºå‰10å®¶åº—é“º
            print(f"\nå‰10å®¶åº—é“ºä¿¡æ¯ï¼š")
            for i, store in enumerate(ippudo_stores[:10], 1):
                print(f"\n{i}. {store['store_name']}")
                print(f"   URL: {store['url']}")
                content_preview = store['content'][:150].replace('\n', ' | ')
                print(f"   å†…å®¹: {content_preview}...")
    
    # è¯¢é—®æ˜¯å¦ä¿å­˜æµ‹è¯•æ•°æ®
    print(f"\n{'='*80}")
    save_data = input("æ˜¯å¦ä¿å­˜æµ‹è¯•æ•°æ®åˆ°æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
    if save_data == 'y':
        output_file = 'data/test_ippudo_stores.json'
        os.makedirs('data', exist_ok=True)
        
        ippudo_stores = [article for article in scraper.articles 
                        if 'ippudo' in article.get('tags', []) 
                        and article.get('section') == 'Store Information']
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(ippudo_stores, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   å…±ä¿å­˜ {len(ippudo_stores)} å®¶åº—é“ºä¿¡æ¯")
    
    print(f"\n{'='*80}")
    print("æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*80}")


def test_extract_directory_links():
    """æµ‹è¯•æå–ç›®å½•é“¾æ¥åŠŸèƒ½"""
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•æå–ç›®å½•é“¾æ¥åŠŸèƒ½")
    print("=" * 80)
    print()
    
    scraper = RamenScraper()
    
    # æµ‹è¯•ä¸»é¡µ
    test_url = "https://stores.ippudo.com/en/japan"
    print(f"æµ‹è¯•URL: {test_url}")
    print()
    
    html = scraper.get_page(test_url, delay=0.6)
    if not html:
        print("âŒ æ— æ³•è·å–é¡µé¢")
        return
    
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')
    
    links = scraper.extract_directory_links(soup, test_url)
    
    print(f"âœ… æ‰¾åˆ° {len(links)} ä¸ªç›®å½•é“¾æ¥")
    print()
    
    if links:
        print("å‰10ä¸ªé“¾æ¥ï¼š")
        for i, link in enumerate(links[:10], 1):
            print(f"{i}. {link['name']}: {link['url']}")
    
    print(f"\n{'='*80}")
    print("æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*80}")


def test_parse_store_detail():
    """æµ‹è¯•è§£æåº—é“ºè¯¦æƒ…é¡µé¢"""
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•è§£æåº—é“ºè¯¦æƒ…é¡µé¢")
    print("=" * 80)
    print()
    
    scraper = RamenScraper()
    
    # æµ‹è¯•ä¸€ä¸ªåº—é“ºè¯¦æƒ…é¡µé¢ï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„åº—é“ºURLï¼‰
    test_url = "https://stores.ippudo.com/en/1813"  # ç¤ºä¾‹URLï¼Œå¯èƒ½éœ€è¦æ›¿æ¢
    print(f"æµ‹è¯•URL: {test_url}")
    print()
    
    html = scraper.get_page(test_url, delay=0.6)
    if not html:
        print("âŒ æ— æ³•è·å–é¡µé¢")
        return
    
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, 'html.parser')
    
    store_data = scraper.parse_ippudo_store_detail(soup, test_url, "æµ‹è¯•éƒ½é“åºœçœŒ")
    
    if store_data:
        print("âœ… æˆåŠŸè§£æåº—é“ºä¿¡æ¯ï¼š")
        print()
        print(f"åº—é“ºåç§°: {store_data['store_name']}")
        print(f"URL: {store_data['url']}")
        print(f"å†…å®¹:")
        print(store_data['content'])
    else:
        print("âŒ æœªèƒ½è§£æåº—é“ºä¿¡æ¯")
    
    print(f"\n{'='*80}")
    print("æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*80}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æµ‹è¯•ä¸€é£å ‚åº—é“ºçˆ¬å–åŠŸèƒ½',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python3 test_ippudo_stores.py                    # å®Œæ•´æµ‹è¯•
  python3 test_ippudo_stores.py --quick             # å¿«é€Ÿæµ‹è¯•ï¼ˆä»…æµ‹è¯•å•ä¸ªéƒ½é“åºœçœŒï¼‰
  python3 test_ippudo_stores.py --test-links        # æµ‹è¯•æå–ç›®å½•é“¾æ¥
  python3 test_ippudo_stores.py --test-detail       # æµ‹è¯•è§£æåº—é“ºè¯¦æƒ…
        """
    )
    
    parser.add_argument('--quick', action='store_true',
                       help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆä»…æµ‹è¯•å•ä¸ªéƒ½é“åºœçœŒï¼‰')
    parser.add_argument('--test-links', action='store_true',
                       help='ä»…æµ‹è¯•æå–ç›®å½•é“¾æ¥åŠŸèƒ½')
    parser.add_argument('--test-detail', action='store_true',
                       help='ä»…æµ‹è¯•è§£æåº—é“ºè¯¦æƒ…åŠŸèƒ½')
    
    args = parser.parse_args()
    
    try:
        if args.test_links:
            test_extract_directory_links()
        elif args.test_detail:
            test_parse_store_detail()
        elif args.quick:
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
            print("=" * 80)
            print("ğŸ§ª å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä»…æµ‹è¯•å•ä¸ªéƒ½é“åºœçœŒ")
            print("=" * 80)
            print()
            
            scraper = RamenScraper()
            test_prefecture_url = "https://stores.ippudo.com/en/japan/æ±äº¬éƒ½"
            
            print(f"ğŸ“Œ æµ‹è¯•URL: {test_prefecture_url}")
            print()
            
            visited_urls = set()
            stores = scraper.scrape_ippudo_stores_recursive(
                test_prefecture_url,
                "æ±äº¬éƒ½",
                visited_urls,
                max_depth=5
            )
            
            print(f"\n{'='*80}")
            print(f"âœ… æµ‹è¯•å®Œæˆï¼")
            print(f"   æ‰¾åˆ° {len(stores)} å®¶åº—é“º")
            print(f"{'='*80}")
            
            if stores:
                print("\nåº—é“ºä¿¡æ¯ï¼š")
                for i, store in enumerate(stores, 1):
                    print(f"\n{i}. {store['store_name']}")
                    print(f"   URL: {store['url']}")
                    content_preview = store['content'][:150].replace('\n', ' | ')
                    print(f"   å†…å®¹: {content_preview}...")
        else:
            test_ippudo_stores(quick_mode=False)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œæ‰§è¡Œå®Œæ•´æµ‹è¯•
    if len(sys.argv) == 1:
        test_ippudo_stores()
    else:
        main()

